---
typora-copy-images-to: assert
typora-root-url: assert
---

[TOC]

```
学习网址：
k8s中文社区：https://www.kubernetes.org.cn/
k8s社区文档：http://docs.kubernetes.org.cn/

k8s官网：https://kubernetes.io/zh/
k8s官方文档：https://kubernetes.io/zh/docs/home/

kubesphere官方文档：https://kubesphere.io/zh/docs/
kubesphere文档中心：https://v2-1.docs.kubesphere.io/docs/zh-CN/release/release-v211/

```



# 三个时代

## 传统时代

 ![1651411160134](/assert/1651411160134.png)

```json
描述：
	在硬件上搭建操作系统，部署多个APP应用
缺点：
	资源应用不充分不隔离，部署的复杂性（迁移时所有环境都需要重新搭建，可以整个系统克隆）
```

## 虚拟化时代

 ![1651411223299](/assert/1651411223299.png)

```json
描述：
	在操作系统上搭建多个虚拟机，在每个虚拟机上部署多个APP应用
优点：
	虚拟机间资源隔离，虚拟机间互不影响，虚拟机可以做成镜像复制使用
缺点：
	资源浪费，每个虚拟机都有一个完整的系统和环境；虚拟机启动慢
```

## 容器化时代（Docker）

 ![1651589290283](/assert/1651589290283.png)

```
描述：
	在操作系统上搭建容器运行时环境，拉取镜像创建容器，每个容器只包含运行时所需环境
优点：
	容器打包成镜像复制使用很方便，容器间运行时环境隔离，资源节约；容器启动迅速；容易实现大量的分布式部署、扩展
缺点：
	
```

![1651590070797](/assert/1651590070797.png)

# K8S简介

```json
容器编排，在多个容器运行时环境里动态的上线容器、下线容器

docker swarm：只能编排docker运行时环境

K8S：K8S是部署和管理分布式环境的，是一个编排系统，用于构建一个强大、健壮、弹性和可扩展性的分布式系统
可以管理多种不同类型的容器运行时环境；
```

docker swarm：docker官方的编排工具，例如可以保证在docker集群中启动3个购物车容器，如果某环境宕机了，还可以动态在其他环境再启动一个

 ![1651590205032](/assert/1651590205032.png)

## 功能

### 调度

```
决定服务在哪个节点启动
```

![1651590841425](/assert/1651590841425.png)

### 自动恢复

```
检查节点不能响应，自动再其他节点启动
```

![1651590897207](/assert/1651590897207.png)

### 水平伸缩

```
应用负载较高时，自动再启动一个应用
```

![1651590882952](/assert/1651590882952.png)

## 架构

### 整体主从

```
Master+Node，每个节点包含Docker运行时环境+kubelet
```

![1651591094929](/assert/1651591094929.png)

### 主体结构

```
UI：可视化界面
CLI：命令行，通过Master暴露的API请求K8S集群，由Master调用各Node完成响应
Master：
Node：
```

![1651591227331](/assert/1651591227331.png)

#### Master架构

```
每个部分都可以搭建集群
1.API Server接收部署请求，存储到etcd中
2.scheduler监控etcd，选择节点启动新创建未运行的pod
```

![1651591408206](/assert/1651591408206.png)

![1651673333646](/assert/1651673333646.png)

---

---

---

![1651591710738](/assert/1651591710738.png)

![1651591864853](/assert/1651591864853.png)

##### kubectl

![1651673556099](/assert/1651673556099.png)

示例：

![1651673576129](/assert/1651673576129.png)

```
kubectl：命令行工具，用于发送命令行给主节点的【主节点可以接受用户界面、命令行的请求】
```

![1651672591612](/assert/1651672591612.png)

##### Controller

```
主要是定义Pod属性
Pod副本数量
Pod类型
```

##### Deployment

```
部署，按照Controller的定义来部署维护Pod
```

##### service

```
service：一组Pod，例如多个购物车Pod，访问购物车请求访问Pod，service负载决定使用哪个pod
```

![1651672787602](/assert/1651672787602.png)

![1651672993916](/assert/1651672993916.png)

##### Label

```

```

![1651673120036](/assert/1651673120036.png)

![1651673197750](/assert/1651673197750.png)





#### Node架构

```
Node：节点组件
kubelet：代理（真正干活的），保证容器运行在pod中，CSI：每个容器的挂载目录；CNI：每个容器的网络
	创建销毁pod
	pod：一个pod包含一组容器（容器理解为docker中的容器）【多个互相依赖的容器组成一个pod单位】
kube-proxy：例如管理pod的ip，相当于网卡路由器，接收请求转发，将请求负载均衡到POD上（外界通过proxy来访问）
```

![1651591991935](/assert/1651591991935.png)

![1651591941687](/assert/1651591941687.png)

![1651592012149](/assert/1651592012149.png)

## 流程叙述

```
1.通过
```



# kubectl重要命令（K8S）

指令：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands



查看k8s下所有资源（pod、service、deploy、副本）：kubectl get all
查看k8s下所有资源：kubectl get all -o wide

---
查看集群内所有节点：kubectl get nodes

---
查看名称空间：kubectl get ns

---
查看kubelet日志：journalctl -u kubelet
master监控Node各节点：watch kubectl get pod -n kube-system -o wide
查看日志：kubectl logs --namespace kubesphere-devops-system

---
查看所有service：kubectl get svc
查看所有service：kubectl get svc -o wide
删除service：kubectl delete service/tomcat6

---
查看所有名称空间的pod：kubectl get pods --all-namespaces
查看所有名称空间的pod：kubectl get pods --all-namespaces -o wide
查看默认名称空间pods：kubectl get pods
查看指定名称空间pod：kubectl get pods --namespace kube-system -o wide
查询pod详细信息：kubectl --namespace kube-system describe pod tiller-deploy-7b76b656b5-qpmb4
查看helm： kubectl get pod -n kube-system -l app=helm
查看tiller：kubectl get pods -n kube-system -owide | grep tiller-deploy
删除pod：kubectl delete pod/tomcat6

---
查询部署：kubectl get deployment
查询名称空间下的部署：kubectl get deployment -n kube-system
创建一个部署：kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8
暴露一个部署。service随机分配端口：kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort
**获取部署的yaml**：kubectl create deployment tomcat7 --image=tomcat:6.0.53-jre8 --dry-run -o yaml
应用升级：kubectl set image
设置某部署副本数，pod数：kubectl scale --replicas=3 deployment tomcat6
删除名称空间下的部署：kubectl delete deployment tiller-deploy  --namespace kube-system（要删除部署，不要删除pod，否则会自动部署pod）
删除部署：kubectl delete deployment.apps/tomcat6

---
获取help信息（可以看到--dry-run -o yaml生成模板的提示）：kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8 --help（可以看到--dry-run -o yaml生成模板的提示）
生成模板到文件tomcat6.yaml：kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8 --dry-run -o yaml > tomcat6.yaml
使用yaml完成部署：kubectl apply -f tomcat6.yaml


```
异常解决：
2.master执行watch kubectl get pod -n kube-system -o wide发现问题
文档：https://www.jianshu.com/p/bcc05427990d（CrashLoopBackOff）
	1）找到出问题的主机执行kubectl get pod获取pod的状态
	2）查看pod的详细信息：kubectl describe pod elkhost-944bcbcd4-8n9nj
	3）查看此pod日志：kubectl logs elkhost-944bcbcd4-8n9nj
```




# 一、安装kubernetes集群

```
方式一：安装minikube
方式二：k8s集群安装
```

方式一：简述

![1652070154659](/assert/1652070154659.png)

方式二：前置要求

![1652103111809](/assert/1652103111809.png)

## 1.基础环境搭建

【部署步骤】

![1641648625504](/assert/1641648625504.png)

### 1.1.vagrant创建虚拟机环境（NAT网络）

查看虚拟机网卡：

![1641648689796](/assert/1641648689796.png)

```
1.拷贝k8s文件夹到一个无空格、中文的目录下，修改配置文件Vagrantfile：node.vm.box = "centos7"
	原因：默认使用centos/7，会去仓库下载镜像（耗时很长），所以使用本地镜像centos7（在环境搭建.md里，构建了本地镜像centos7，详细请参考环境搭建.md【vagrant box add centos7 CentOS-7-x86_64-Vagrant-2004_01.VirtualBox.box】）
	
2.虚拟机默认配置下存在的问题
	1）无法使用ssh远程连接
		【修复流程】
		vagrant ssh k8s-node1
		su root   密码：vagrant
		vi /etc/ssh/sshd_config	修改PasswordAuthentication yes
		service sshd restart（或者重启：reboot）
		使用xshell工具连接，连接信息：192.168.56.100  root/vagrant
	
	2）三台虚拟机共用同一IP
		【问题描述】
		默认情况下存在以下两个网卡：
			网卡一：eth0（网络地址转换NAT，所有虚拟机共用1个IP【会导致集群环境搭建困难度】，此种网络环境是通过映射主机端口与虚拟机端口完成通讯的【同时映射多台虚拟机】）
			网卡二：eth1（仅主机Host-only网络，192.168.56.100）【方便开发，主机环境ssh连接虚拟机】
	
	--命令ip route show：查看每个虚拟机的所有网卡和默认使用的网卡（eth0）
	--命令ip addr：查看每个虚拟机的网卡以及IP（三台虚拟机eth0网卡下的IP都是10.0.2.215）
	
		【解决方案】：
		网卡1：使用NAT网络代替  网络地址转换NAT
		网卡2：维持默认，使用仅主机Host-only网络，保证宿主机与k8s虚拟机在同一局域网内192.168.56.xx（虚拟机私有网络）
		
		【具体步骤】
		1）将三台虚拟机的电源关闭
		2）在virtual box管理器上依次点击：工具=》全局设定=》网络=》添加新的NAT网络
		3）选中一台虚拟机=》设置=》网络=》网卡1=》连接方式：NAT网络=》界面名称：选择步骤2创建的NAT网络=》高级=》刷新mac地址（每台虚拟机都会新生成一个独立IP）
		4）每台虚拟机重复步骤3
		5）设置成功后记录每台虚拟机网卡1和网卡2的ip，以下是我本地的ip，供参考
			192.168.56.100=》10.0.2.4
			192.168.56.101=》10.0.2.15
			192.168.56.102=》10.0.2.5

3.配置linux环境
关闭防火墙:
	systemctl stop firewalld
	systemctl disable firewalld
关闭selinux:（默认安全策略 cat /etc/selinux/config）
	sed -i 's/enforcing/disabled/' /etc/selinux/config
	setenforce 0
关闭swap: 关闭内存交换，否则k8s性能会出现问题（cat /etc/fstab）
	swapoff -a	只是临时关闭
	sed -ri 's/.*swap.*/#&/' /etc/fstab   使用此命令永久关闭
	
4.修改主机名
	1）查看主机名，如果主机名是localhost需要修改
		--命令：hostname查看当前主机名
	2）修改主机名
		hostnamectl set-hostname k8s-node1

5.修改每个虚拟机的hosts，保证各节点间可以通过域名访问
vi /etc/hosts
10.0.2.4 k8s-node1
10.0.2.15 k8s-node2
10.0.2.5 k8s-node3

6.将桥接的IPv4流量传递到iptables的链，每个节点都执行以下命令
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system

7.备份：
在virtualBox上给三台主机做备份

疑难问题:
遇见提示是只读的文件系统，运行如下命令
mount -o remount rw /

date查看时间(可选)
yum install -y ntpdate
ntpdate time.windows.com同步最新时间



```

### 1.2.安装容器运行时环境（docker）

```json
简述：k8s集群环境，每个节点都需要一个容器运行时环境（这里选择docker）

1、卸载旧版本【uninstall old versions】
	sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
                  
2、安装前置依赖
sudo yum install -y yum-utils device-mapper-persistent-data lvm2

3、设置镜像地址
	方案一：
	sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
	方案二：（推荐，阿里镜像）
	sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo【这个镜像和后面提到的镜像加速不同，这里指的是下载docker本身的镜像地址】
	
4、安装docker以及docker-cli
sudo yum install -y docker-ce docker-ce-cli containerd.io

5、虚拟机开机启动：
	sudo systemctl enable docker
启动docker
	sudo systemctl start docker


6、测试
docker images -a

7、docker配置镜像下载加速
	1）默认从hub.docker下载软件镜像【很慢】
	2）修改成aliyun的镜像加速器
	3）执行【获取第一步配置的镜像加速器网址】
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://7zi5cb9i.mirror.aliyuncs.com"]
}
EOF

sudo systemctl daemon-reload
sudo systemctl restart docker
	
8.拉取
docker pull quay.io/openebs/provisioner-localpv:1.5.0
```



### 1.3.安装kubeadmin、kubelet、kubectl

![1644334589330](/assert/1644334589330.png)

```json
1.每个节点安装kubeadmin（快速构建集群环境的工具，主节点设置Master，从节点join到集群）
2.每个节点安装kubelet（真正干活的）
3.每个节点安装kubectl（命令行工具）
4.每个节点都要安装Docker（master和Node都要，容器运行时环境）
```

```
添加阿里云yum源：（Kubernetes安装地址）

cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

```
查看是否安装相关源：yum list|grep kube
1、安装
yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3

2、开机启动kubelet，要把每一个Node注册到集群中
sudo systemctl enable kubelet
sudo systemctl start kubelet【到这一步暂时还启动不起来，使用systemctl status kubelet查看状态，后面还有要配置的】
```

### 重置集群（非必要）

```sh
如果在安装需要重新安装，且执行了以下两行语句，重新安装会失败，需要执行一些删除操作（如果没有重装的需求，跳过此步骤）
swapoff -a
kubeadm reset
	文档：https://blog.csdn.net/woay2008/article/details/93250137
-----------------------------------------------------------------------------------------
需要执行以下操作：

Master执行：
systemctl daemon-reload
systemctl restart kubelet
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X  
rm -rf $HOME/.kube

Master+Node执行：
     rm -rf /var/lib/etcd
	 rm -rf /etc/cni/net.d
	 rm -rf /var/lib/kubelet
	 rm -rf /etc/kubernetes
```

## 2.部署

### 2.1.部署Master

```
1.挑选一台主机为master（当前选择10.0.2.4作为主机），将k8s文件夹上传到master主机的root路径下（gitee上有）

2.打开k8s文件，执行可执行文件拉取镜像
	1）赋予执行权限（rwx）：chmod 700 master_images.sh
	2）执行：./master_images.sh
【No such file 解决方法：https://blog.csdn.net/youzhouliu/article/details/79051516】
	3）查看镜像下载情况：docker images

3、初始化master节点上，在master上执行：
sudo kubeadm init \
--apiserver-advertise-address=10.0.2.4 \
--image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
--kubernetes-version v1.17.3 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=10.244.0.0/16

打印Your Kubernetes control-plane has initialized successfully!代表初始化成功

讲解：
	1.apiserver-advertise-address使用主节点ip（ip addr查看，需要使用默认网卡地址 eth0）
	2.默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。可以手动按照我们的执行master_images.sh先拉取镜像，地址变为registry.aliyuncs.com/google_containers也可以。
	3.pod是k8s中最小的单位（多个容器组成）；service是一组pod组成对外提供服务，负载均衡
	4.cidr：无类别域间路由(Classless Inter-Domain Routing、CIDR)是一个用于给用户分配IP地址以及在互联网上有效地路由IP数据包的对IP地址进行归类的方法。【1个Node有多个service，1个service有多个pod，1个pod有多个容器】
拉取可能失败，需要下载镜像。

4.记录上一步骤执行成功提示的内容：（如果token过期，重新获取一个没有过期时间的token（默认2h过期）
kubeadm token create --print-join-command --ttl=0）
	1.配置文件
	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config
	
	2.pod网络配置，使用Flannel，是一个可以用于 Kubernetes 的 overlay 网络提供者。
	文档：https://kubernetes.io/docs/concepts/cluster-administration/addons/

	3.从节点加入集群：
kubeadm join 10.0.2.4:6443 --token rbbfs6.or9h6i8mgdfm1czl \
    --discovery-token-ca-cert-hash sha256:9000449fd492240244794f13aab5dbd99e939dc80650b378ebee27e9c0a7f851 


5.主节点执行，生成配置文件：
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

**master镜像拉取：**

![1655616714880](/assert/1655616714880.png)

**初始化成功：**

![1655617942795](/assert/1655617942795.png)

### 2.2.安装pod网络插件（CNI）

![1641658946879](/assert/1641658946879.png)

```
部署pod网络【只有部署了网络，Node才可以加入进来】
参照doc：https://kubernetes.io/docs/concepts/cluster-administration/addons/
使用Flannel： 是一个可以用于 Kubernetes 的 overlay 网络提供者。


执行
kubectl apply -f \
https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
这个地址被墙，使用我们k8s文件夹中的yml即可，同时flannel.yml中指定的images访问不到可以去docker hub找一个

1、使用本地的flannel.yml【先拉取镜像】
	1）如果kube-flannel.yml中指定的images访问不到去docker.hub中查找并修改本地yml中的配置
		# 1、访问hub.docker.com，搜索flannel，点击from quay.io/coreos/flannel
		# 2、拷贝v0.11.0-amd64的来源jmgao1983/flannel:v0.11.0-amd64
		# 3、flannel.yml中image指定的地方，修改
	2）也可以使用quay-mirror.qiniu.com和registry.aliyuncs.com
	例如quay.io=》quay-mirror.qiniu.com  || gcr.io改为registry.aliyuncs.com
		# 下拉镜像：quay.io/coreos/flannel:v0.10.0-s390x
		  如果拉取较慢，可以改为：quay-mirror.qiniu.com/coreos/flannel:v0.10.0-s390x
		# 下拉镜像：gcr.io/google_containers/kube-proxy
		  如果拉取较慢，可以改为： registry.aliyuncs.com/google_containers/kube-proxy

2、执行，使用本地的yml规则安装pod组件（kubectl delete -f kube-flannel.yml：删除yml中配置的所有组件重新安装）
	kubectl apply -f kube-flannel.yml


3、查看名称空间
kubectl get ns

4、获取所有名称空间下的的pod（相当于docker ps）
kubectl get pods --all-namespaces -o wide
如果全部都是running就成功了，如果有不是running状态的等待一段时间，下载好镜像后就可以了

docker images|grep flannel可以查到以下镜像：
jmgao1983/flannel                                                             v0.11.0-amd64       ff281650a721        19 months ago       52.6MB

```

**kube-flannel.yml：**

 ![1655619067179](/assert/1655619067179.png)

### 2.3.部署Node（将Node加入K8S集群）

```
1、当master是ready状态时，在k8s-node2、k8s-node3执行以下命令将其添加到master
kubeadm join 10.0.2.4:6443 --token rbbfs6.or9h6i8mgdfm1czl \
    --discovery-token-ca-cert-hash sha256:9000449fd492240244794f13aab5dbd99e939dc80650b378ebee27e9c0a7f851 

异常解决：
2.master执行watch kubectl get pod -n kube-system -o wide发现问题
文档：https://www.jianshu.com/p/bcc05427990d（CrashLoopBackOff）
	1）找到出问题的主机执行kubectl get pod获取pod的状态
	2）查看pod的详细信息：kubectl describe pod elkhost-944bcbcd4-8n9nj
	3）查看此pod日志：kubectl logs elkhost-944bcbcd4-8n9nj
	
3、master监控Node各节点
watch kubectl get pod -n kube-system -o wide
查看到node2和node3镜像下载失败
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.17.3
docker pull kubesphere/kube-proxy:v1.17.3
docker pull jmgao1983/flannel:v0.11.0-amd64
```

### 异常处理

```
k8s从节点报错：The connection to the server localhost:8080 was refused - did you specify the right host or port?

文档：https://www.cnblogs.com/wangzy-Zj/p/13958054.html

1、复制配置文件
scp root@主节点服务器地址:/etc/kubernetes/admin.conf /etc/kubernetes/

2、添加环境变量
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile

3、申明环境变量
source ~/.bash_profile

4、重新查看配置
kubectl get nodes
```



## 3.入门操作体验

### 部署tomcat（容灾恢复）

```
1.根据镜像创建一个部署：根据tomcat镜像（带java环境的镜像）创建一个部署（名字叫tomcat6）
kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8
	解析：k8s会通过主节点调用其他节点下载镜像启动容器（1个部署对应1个pod对应1个容器）
	

Kubectl get pods -o wide可以获取到tomcat信息【Kubectl get all】
# 容灾恢复1：在k8s-node3执行docker stop tomcat6停掉该容器，kubernetes会给node3重启一个容器
# 容灾恢复2：如果k8s-node3宕机，master会找到集群中另一个节点启动pod
```



### 暴露nginx访问

```
2、暴露一个部署【创建service】
kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort

解释：暴露一个tomcat6的部署，容器的8080端口映射pod的80端口，然后使用NodePort的方式【service为pod的80端口随机分配一个端口提供服务】。也可以不使用--type使用--node-port=31533
192.168.56.101:31533访问tomcat6

解释：暴露一个service出去，给service随机分配一个端口。此端口映射pod:80，pod:80端口映射容器8080

使用命令查看service端口：然后通过 192.168.56.101:31533可以访问Tomcat
kubectl get svc -o wide
```



### 动态扩容（缩容）测试

```
3、动态扩容测试
kubectl get deployment
应用升级：kubectl set image
扩容哪个部署：kubectl scale --replicas=3 deployment tomcat6

使用 kubectl get pods -o wide 查看扩容结果
结果：在k8s-node2部署了2个pod，在k8s-node3部署了1个pod，一共3个。并且service对外暴露的接口都是31533
192.168.56.101:31533
192.168.56.102:31533 都可以访问tomcat6

注意：如果需要缩容，执行：kubectl scale --replicas=1 deployment tomcat6
```

扩容：

![1655646001938](/assert/1655646001938.png)

缩容：两台terminating

![1655646081076](/assert/1655646081076.png)

### 删除

```
1.删除部署：会同步删除pods，不会删除service（但service已经无用处）
	kubectl delete deployment apps/tomcat6

2.删除service：
	kubectl delete service/tomcat6
```

# 二、kubectl与yaml语法的作用

## 1.kubectl指令

```
可以通过指令创建部署、暴露端口、获取节点信息等操作

官方文档：
https://kubernetes.io/zh/docs/reference/kubectl/overview/
https://kubernetes.io/zh-cn/docs/reference/kubectl/
```

## 2.yaml语法

```
也可以通过yaml配置文件的形式，创建部署
```

![1655647599009](/assert/1655647599009.png)

![1655647750772](/assert/1655647750772.png)

```
之前使用kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8命令行的方式部署tomcat6，现在只需要kubectl apply -f example-controller.yaml就可以实现一次部署了

1、使用yaml的方式完成部署
	1）生成一段yaml模板到tomcat6.yaml
	获取提示：kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8 --help
	生成模板文件：kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8 --dry-run -o yaml > tomcat6.yaml
	2）使用模板文件部署
	kubectl apply -f tomcat6.yaml
	3）查看pods
	kubectl get pods，
	
2、使用yaml的方式完成暴露部署
	1）生成一段yaml模板到tomcat6_expose.yaml
	kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml
	kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml > tomcat6_expose.yaml
	2）使用该yaml完成暴露
	kubectl apply -f tomcat6_expose.yaml
	
3、使用yaml的方式创建pod
	kubectl get pod tomcat6-5f7ccf4cb9-kb5hv -o yaml > mypod.yaml
如下图，创建两个image。
	kubectl apply -f mypod.yaml
	kubectl get pods : 0/2 获取到tomcat6-new 有2个容器
	
```

![1655648326914](/assert/1655648326914.png)

# 三、K8S细节

## 1.pod、controller、service简介

```
Pod是Kubernetes应用程序的基本执行单元，即它是Kubernetes对象模型中创建或部署的最小和最简单的单元。Pod表示在集群上运行的进程。

controller：（POD以何种方式部署【例如有状态部署、每个节点都需要的部署】）
	ReplicationController：过时
	ReplicaSet：取代了ReplicationController，用于副本复制
	Deployments：
	StatefulSets：有状态的部署【mysql有状态部署】
	DaemonSet：每一个节点都需要启动

service：
	service是对pod的负载均衡，pod可以使用不同的端口，然后整个service提供一个单独的端口为外接提供服务。

```

**节点组成：**

![1655648975992](/assert/1655648975992.png)

## 2.pod与service的关系

![1655649781638](/assert/1655649781638.png)

## 3.service的意义

```
统一应用的访问入口

一般是用于暴露的
```

![1655649840625](/assert/1655649840625.png)

## 4.label and selector

```
可以给pod打标签A和B，创建两个部署，分别选中A和B
```

![1655649948438](/assert/1655649948438.png)

## 5.ingress（nginx）

b站349集：https://www.bilibili.com/video/BV1np4y1C7Yf?p=350

```
作用：service可以实现节点ip:port的方式访问应用，ingress可以做到域名的方式访问（负载均衡）【底层是一个nginx】

1）部署Ingress Controller
	DaemonSet：每个节点都部署ingress，相当于为每个节点都暴露了80、443等端口（pod是ingress-nginx）
2）创建Ingress规则：在暴露服务的时候指定ingress规则
	保证使用域名就可以访问到service
	serviceName：tomcat6
	servicePort：80（pod端口，而不是service端口）
3）在windows主机本地修改本地DNS绑定
	192.168.56.102 tomcat6.atguigu.com
```

 ![1655653508251](/assert/1655653508251.png)

2）创建Ingress规则

 ![1655653896997](/assert/1655653896997.png)

之前暴露service的yaml：

 ![1655654262788](/assert/1655654262788.png)

```yaml
1）部署Ingress Controller

apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: apps/v1
kind: DaemonSet 
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      hostNetwork: true
      serviceAccountName: nginx-ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: siriuszg/nginx-ingress-controller:0.20.0
          imagePullPolicy: IfNotPresent
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10

---
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
spec:
  #type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP
  - name: https
    port: 443
    targetPort: 443
    protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

```



## 6.网络模型



# 四、安装kubesphere

## 1.可视化界面

```
介绍三个可视化界面：
1.dashboard：kubernetes默认可视化界面
2.kuboard：轻量级可视化界面，集群要求不高 https://kuboard.cn/support
3.kubesphere：集群要求较高，可以打通devops链路 https://kubesphere.com.cn/
```

1.默认可视化工具安装：使用kubernetes-dashboard.yaml 部署（在k8s文件夹内）

 ![1655654758600](/assert/1655654758600.png)

2.暴露服务

 ![1655654856214](/assert/1655654856214.png)

3.创建授权账户

 ![1655654869401](/assert/1655654869401.png)

登录界面：

 ![1655654909654](/assert/1655654909654.png)

## 2.kubesphere简介

 ![1655654962955](/assert/1655654962955.png)

```
KubeSphere是一款面向云原生设计的开源项目，在目前主流容器调度平台 Kubernetes之上构建的分布式多租户容器管理平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。

安装kubesphere前需要安装helm、tiller
helm：客户端（简化部署指令，kubesphere依赖此客户端，实现快速部署应用）
例如创建一个部署命令  kubectl create deployment xxx，可以直接替换为 helm install xxx

tiller：服务端
```

## 3.安装helm、tiller

文档中心：https://v2-1.docs.kubesphere.io/docs/zh-CN/installation/prerequisites/

资料：https://www.cnblogs.com/xiao987334176/p/13267339.html

helm安装被墙，处理方式：https://blog.csdn.net/qianghaohao/article/details/98851147

helm安装指南：https://zhuanlan.zhihu.com/p/77496043

helm多种安装方式：http://www.coderdocument.com/docs/helm/v2/using_helm/installing_helm.html

安装tiller指南：https://zhuanlan.zhihu.com/p/77496043

```
方法一：
	1）使用sh文件下载【需要翻墙】：curl -L https://git.io/get_helm.sh | bash
	2）chmod 700 get_helm.sh
	3）./get_helm.sh

方法二：
	1）下载安装包：https://github.com/helm/helm/tags
	2）解压 tar -zxvf helm-v2.16.3-linux-amd64.tar.gz
	3）mv /root/k8s/linux-amd64/helm /usr/local/bin/helm
	   mv /root/k8s/linux-amd64/tiller /usr/local/bin/tiller
	4）helm version（查看是否安装成功，helm help查看帮助）

2、创建服务账号和角色绑定
cd /root/k8s
vi helm-rbac.yaml，把下面内容加到yaml中

apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system

3、部署：kubectl apply -f helm-rbac.yaml

4、初始化tiller：
	（推荐，如果有异常，通过修改dns解决）：helm init --service-account tiller --upgrade -i registry.aliyuncs.com/google_containers/tiller:v2.16.3
	
	注意：上面拉取镜像失败时，再执行下面一句，就可以了
	helm init --service-account tiller --upgrade -i gcrxio/tiller:v2.16.3 --stable-repo-url=https://mirror.azure.cn/kubernetes/charts/
 	【--tiller-image指定镜像，否则会被墙】

5、检查tiller安装是否成功【如果安装失败，参考命令集合，查看pod细节 describe】
【这四个都可以查是否安装成功】
tiller
helm version
kubectl get deployment -n kube-system【看部署】
kubectl get pod -n kube-system -l app=helm【看pod】
kubectl get pods --all-namespaces【看所有pods】
```

```
修改dns：【解决docker拉取失败】
vi /etc/resolv.conf
nameserver 114.114.114.114将原来的覆盖掉
不需要重启网络，否则会将新的dns覆盖掉

也可以在centos7中永久性的修改dns，但感觉速度不是很好
cd /etc/sysconfig/network-scripts/
ls
打开设置了静态ip的网卡文件，如ifcfg-enp0s8

在PEERDNS="no"下面添加
DNS1=8.8.8.8
DNS2=114.114.114.114
【
PEERDNS="no"
DNS1=8.8.8.8
DNS2=114.114.114.114
】

重启网络
sudo service network  restart

查看dns是否修改
cat /etc/resolv.conf
```

初始化成功：

![1656233913054](/assert/1656233913054.png)

## 4.安装存储类型OpenEBS（StorageClass）

```
kubesphere是有状态应用，依赖存储类型（StorageClass）

OpenEBS：仅仅是存储类型中的一种，他会提供PV给kubesphere的有状态应用挂载，因此，若master节点存在Taint（污点），建议在安装OpenEBS前取消Taint，待OpenEBS安装完成后再对master打上Taint
```

```sh
1.获取master节点信息：
	kubectl get node -o wide

2.获取master节点下的Taint（污点表示有人在调度？）
	kubectl describe node k8s-node1 | grep Taint
	
3.删除污点
	kubectl taint nodes k8s-node1 node-role.kubernetes.io/master:NoSchedule-
	
4.安装OpenEBS（http://docs.openebs.io/）
	1）创建 OpenEBS 的 namespace，OpenEBS 相关资源将创建在这个 namespace 下：
		kubectl create ns openebs
		kubectl get ns
	2）安装
		helm install --namespace openebs --name openebs stable/openebs --version 1.5.0
	
	BUG1：
	helm repo remove stable
	helm repo add stable https://kubernetes-charts.storage.googleapis.com
	helm repo update
	helm search
	然后重新安装：
	helm install --namespace openebs --name openebs stable/openebs --version 1.5.0
	
	BUG2：镜像下载失败【重新设置一下阿里云加速】
	查看部署
	kubectl get deployment -n openebs
	kubectl --namespace openebs describe pod openebs-ndm-g8mfh
	再下载image【k8s-node2、k8s-node3都下载】
	docker pull openebs/admission-server:1.5.0			
	docker pull openebs/m-apiserver:1.5.0      			
	docker pull openebs/provisioner-localpv:1.5.0		
	docker pull openebs/node-disk-manager-amd64:v0.4.5  【这个在k8s-node1（master）中下载】
	docker pull openebs/node-disk-operator-amd64:v0.4.5 
	docker pull openebs/openebs-k8s-provisioner:1.5.0	
	
	docker pull openebs/snapshot-controller:1.5.0		
	docker pull openebs/snapshot-provisioner:1.5.0		
	然后查看部署状态：【全部running，则安装成功】
	kubectl get deployment -n openebs
	watch kubectl get pods --all-namespaces -o wide【监控，如果全部ready则成功】
	
5、安装 OpenEBS 后将自动创建 4 个 StorageClass，查看创建的 StorageClass
	kubectl get sc
	
6、将 openebs-hostpath设置为 默认的 StorageClass：
	kubectl patch storageclass openebs-hostpath -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

7、OpenEBS 的 LocalPV 已作为默认的存储类型创建成功。可以通过命令 kubectl get pod -n openebs来查看 OpenEBS 相关 Pod 的状态，若 Pod 的状态都是 running，则说明存储安装成功
【污点在kubesphere安装成功后加】
	kubectl taint nodes k8s-node1 node-role.kubernetes.io/master=:NoSchedule
```

准备成功：

![1656237063877](/assert/1656237063877.png)

设置默认存储类型：

![1656237145364](/assert/1656237145364.png)

## 5.最小化安装Kubesphere

```
中国官方安装文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/installation/install-on-k8s/
github安装指南：https://github.com/kubesphere/ks-installer/blob/master/README_zh.md
2.1分支安装指南：https://github.com/kubesphere/ks-installer/blob/v2.1.0/README_zh.md
可插拔功能组件：https://github.com/kubesphere/ks-installer/blob/master/deploy/cluster-configuration.yaml

最小化安装失败：https://kubesphere.com.cn/forum/d/1262-k8s（安装OpenEBS和KubeSphere都要删掉master的污点）
使用NFS存储代替OpenEBS：https://kubesphere.com.cn/forum/d/1272-kubeadm-k8s-kubesphere-2-1-1
```

```
注：当安装了最小化安装后，可通过可插拔的方式继续安装应用
	也可以安装完整版的（鉴于自己的需求）

1.删除污点
	kubectl taint nodes k8s-node1 node-role.kubernetes.io/master:NoSchedule-

2.调整从节点内存16g、8核

3.创建kubesphere-minimal.yaml

4.安装
	kubectl apply -f kubesphere-minimal.yaml
	
5.监控安装日志
	kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f
	kubectl get pods --all-namespaces

6.日志打印控制台 ip:port，即完成
	需要使用 192.168.56.100:30880/login 访问
	账号/密码：admin/P@88w0rd

7.设置污点
	kubectl taint nodes k8s-node1 node-role.kubernetes.io/master=:NoSchedule
```

安装成功：

 ![1656244705817](/assert/1656244705817.png)

**kubesphere-minimal.yaml:**

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: kubesphere-system

---
apiVersion: v1
data:
  ks-config.yaml: |
    ---

    persistence:
      storageClass: ""

    etcd:
      monitoring: False
      endpointIps: 192.168.0.7,192.168.0.8,192.168.0.9
      port: 2379
      tlsEnable: True

    common:
      mysqlVolumeSize: 20Gi
      minioVolumeSize: 20Gi
      etcdVolumeSize: 20Gi
      openldapVolumeSize: 2Gi
      redisVolumSize: 2Gi

    metrics_server:
      enabled: False

    console:
      enableMultiLogin: False  # enable/disable multi login
      port: 30880

    monitoring:
      prometheusReplicas: 1
      prometheusMemoryRequest: 400Mi
      prometheusVolumeSize: 20Gi
      grafana:
        enabled: False

    logging:
      enabled: False
      elasticsearchMasterReplicas: 1
      elasticsearchDataReplicas: 1
      logsidecarReplicas: 2
      elasticsearchMasterVolumeSize: 4Gi
      elasticsearchDataVolumeSize: 20Gi
      logMaxAge: 7
      elkPrefix: logstash
      containersLogMountedPath: ""
      kibana:
        enabled: False

    openpitrix:
      enabled: False

    devops:
      enabled: True
      jenkinsMemoryLim: 2Gi
      jenkinsMemoryReq: 1500Mi
      jenkinsVolumeSize: 8Gi
      jenkinsJavaOpts_Xms: 512m
      jenkinsJavaOpts_Xmx: 512m
      jenkinsJavaOpts_MaxRAM: 2g
      sonarqube:
        enabled: True
        postgresqlVolumeSize: 8Gi

    servicemesh:
      enabled: False

    notification:
      enabled: True

    alerting:
      enabled: True

kind: ConfigMap
metadata:
  name: ks-installer
  namespace: kubesphere-system

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ks-installer
  namespace: kubesphere-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  name: ks-installer
rules:
- apiGroups:
  - ""
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - extensions
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - batch
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - apiregistration.k8s.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - tenant.kubesphere.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - certificates.k8s.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - devops.kubesphere.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - monitoring.coreos.com
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - logging.kubesphere.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - jaegertracing.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - storage.k8s.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - '*'
  verbs:
  - '*'

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ks-installer
subjects:
- kind: ServiceAccount
  name: ks-installer
  namespace: kubesphere-system
roleRef:
  kind: ClusterRole
  name: ks-installer
  apiGroup: rbac.authorization.k8s.io

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ks-installer
  namespace: kubesphere-system
  labels:
    app: ks-install
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ks-install
  template:
    metadata:
      labels:
        app: ks-install
    spec:
      serviceAccountName: ks-installer
      containers:
      - name: installer
        image: kubesphere/ks-installer:v2.1.1
        imagePullPolicy: "Always"
```

## 6.安装可插拔的功能组件

```
最小化安装后安装可插拔的功能组件（根据需要安装）：https://v2-1.docs.kubesphere.io/docs/zh-CN/installation/install-metrics-server/

安装后如何开启 Metrics-server 安装

    通过修改 ks-installer 的 configmap 可以选装组件，执行以下命令。

$ kubectl edit cm -n kubesphere-system ks-installer

参考如下修改 ConfigMap

···
metrics-server:
     enabled: True       

    保存退出，参考 验证可插拔功能组件的安装 ，通过查询 ks-installer 日志或 Pod 状态验证功能组件是否安装成功。


1、Metrics-Server开启HPA【监控CPU使用率 弹性伸缩】：关闭
2、logging日志系统：关闭
3、DevOps系统（Jenkins一站式部署）：开启
	sonarqube：开启
4、Service Mesh微服务治理功能【熔断、限流、链路追踪】【前提是要开启日志】：关闭
5、notification告警通知系统：开启
6、alerting：开启
```

```
1.删除污点

2.修改配置部署文件，实现插拔安装：
	kubectl edit cm -n kubesphere-system ks-installer

3.编辑yaml，开启以下功能，修改结束保存退出即可
	DevOps系统（Jenkins一站式部署）
	sonarqube
	notification
	alerting

4.监控安装过程
	kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f
	kubectl get pods --all-namespaces
	
6.设置污点
	kubectl taint nodes k8s-node1 node-role.kubernetes.io/master=:NoSchedule
```

## 7.kubesphere界面简介

```
1.企业空间：相当于1个团队一个企业空间，互相隔离
	账户会分配企业空间的权限

2.基础设施：可以查看 pod、存储类型、内存、CPU等

3.Monitoring：监控

4.服务组件：

5.提供了控制台：kubectl
```

![1656245416318](/assert/1656245416318.png)

## 8.建立隧道，内网穿透

```
这里使用nps构建内网穿透失败了，可以换其他方案，例如花生壳

作用：远程访问kubesphere控制页，流水线可以根据钩子构建项目
```



# 五、使用kubesphere

## 1.多租户管理_创建账号

文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/admin-quick-start/

![1656336920194](/assert/1656336920194.png)

```
什么是多租户系统？
	对集群资源作权限控制
```



### 角色和账号

| 内置角色           | 描述                                                         |
| :----------------- | :----------------------------------------------------------- |
| cluster-admin      | 集群管理员，可以管理集群中所有的资源。                       |
| workspaces-manager | 集群中企业空间管理员，仅可创建、删除企业空间，维护企业空间中的成员列表。 |
| cluster-regular    | 集群中的普通用户，在被邀请加入企业空间之前没有任何资源操作权限。 |

### 企业空间

| 账号名          | 企业空间角色      | 职责                                                         |
| :-------------- | :---------------- | :----------------------------------------------------------- |
| ws-admin        | workspace-admin   | 管理企业空间下所有的资源 (本示例用于邀请新成员加入企业空间)  |
| project-admin   | workspace-regular | 创建和管理项目、DevOps 工程，邀请新成员加入                  |
| project-regular | workspace-viewer  | 将被 project-admin 邀请加入项目和 DevOps 工程， 用于创建工作负载、流水线等业务资源 |

![1656336968629](/assert/1656336968629.png)

![1659197387815](/assert/1659197387815.png)

### 具体步骤

```
密码：Wan123

1.创建用户管理角色：users-manager
  指派权限：账户管理、角色管理

2.创建用户管理员账户：atguigu-hr/Wan123/atguigu-hr@atguigu.com
  指派角色：users-manager

3.登录atguigu-hr
  创建账号：ws-manager，指派角色：workspaces-manager（作用：创建企业空间+分配企业空间管理员）
  	【工作空间管理员】
  创建账号：ws-admin，指派角色：cluster-regular（作用：邀请企业空间成员+分配成员权限）
  	【企业空间管理员】
  创建账号：project-admin，指派角色：cluster-regular（作用：创建项目+邀请项目成员+分配成员权限）
  	【项目管理员】
  创建账号：project-regular，指派角色：cluster-regular（作用：创建部署、密钥、存储pvc）
  	【普通项目成员】
  
4.登录ws-manager
	1）创建企业空间：gulimall-workspace
	2）指派企业空间管理员：ws-admin
	
5.登录ws-admin
	1）邀请企业成员：（ws-manager也可以邀请企业成员）
		账户：project-admin，指派角色：workspace-regular（作用：创建项目）
		账户：project-regular，指派角色：workspace-viewer（作用：查看项目）
	
6.登录project-admin
	创建项目【测试】
		测试1）创建资源型项目gulimall，并邀请project-regular为该项目的开发人员
		测试2）创建DevOps项目

```



## 2.创建Wordpress应用（普通项目）

```json
https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/wordpress-deployment/

简介：
	WordPress 是使用 PHP 开发的博客平台，用户可以在支持 PHP 和 MySQL 数据库的环境中架设属于自己的网站。本文以创建一个 Wordpress 应用 为例，以创建 KubeSphere 应用的形式将 Wordpress 的组件（MySQL 和 Wordpress）创建后发布至 Kubernetes 中，并在集群外访问 Wordpress 服务。
```

![1656344288732](/assert/1656344288732.png)

```
概念：（service之间网络互通）
	PV:实际的存储资源节点
	PVC:每个应用要申请自己的PVC（持久化存储请求），例如wordpress的前端资源文件，mysql的物理表存储
	SECRET:秘钥，将不方便暴露的数据抽取成一个秘钥，在需要使用的地方引用就可以，例如账号密码
```

资料：

![1659194914196](/assert/1659194914196.png)



![1659195759297](/assert/1659195759297.png)

```
部署步骤：
1.登录project-admin账号，创建资源型项目gulimall
  邀请project-regular成为开发者

2.登录project-regular账号
	2.1.创建 MySQL 密钥
	名称：mysql-secret
	别名：MySQL 密钥
	描述信息：MySQL 初始密码
	类型：选择 默认(Opaque)
	Data：键值对填写 MYSQL_ROOT_PASSWORD 和 123456

注意：这里的键值对的key可以随便写，只需要在配置应用组件的环境变量时，将此秘钥与应用的环境变量作正确映射即可
	
	2.2.创建 WordPress 密钥
	名称：wordpress-secret
	别名：连接 MySQL 密码
	Data 键值对填写 WORDPRESS_DB_PASSWORD 和 123456。
	
3.创建存储卷
	3.1.名称：wordpress-pvc
	别名：Wordpress 持久化存储卷
	描述信息：Wordpress PVC
	完成后点击 下一步，存储类型默认 local【openebs-hostpath】
	访问模式和存储卷容量也可以使用默认值【ReadWriteOnce、10g】，点击 下一步，直接创建即可。
	
	3.2.名称：mysql-pvc
	别名：MySQL 持久化存储卷
	描述信息：MySQL PVC
	完成后点击 下一步，存储类型默认 local【openebs-hostpath】
	访问模式和存储卷容量也可以使用默认值【ReadWriteOnce、10g】，点击 下一步，直接创建即可。
	
4.部署新应用：
	创建Wordpress应用，并同时创建两个应用组件mysql和wordpress
	应用名称：wordpress

	4.1.添加mysql组件
	名称： mysql
	组件版本：v1
	别名：MySQL 数据库
	负载类型：选择 有状态服务
	添加容器镜像，镜像填写 mysql:5.6，点击 使用默认端口
	在高级设置中确保内存限制 ≥ 1000 Mi,否则可能 MySQL 会因内存 Limit 不够而无法启动。
	环境变量，然后选择 引用配置文件或密钥，名称填写为 MYSQL_ROOT_PASSWORD，下拉框中选择密钥为 mysql-secret和 MYSQL_ROOT_PASSWORD。
	点击√
	添加存储卷：选择已有存储卷（之前创建的）、读写
	容器挂载路径：（参考docker启动命令）/var/lib/mysql
	
注意：docker启动命令，docker run --name some-mysql -v /my/own/datadir:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag

	4.2.添加 WordPress 组件
	名称： wordpress
	组件版本：v1
	别名：Wordpress前端
	负载类型：默认 无状态服务
	镜像填写 wordpress:4.8-apache（应指定镜像版本号)，点击 使用默认端口。
	添加两个环境变量：
		点击 引用配置文件或密钥，名称填写 WORDPRESS_DB_PASSWORD，选择在第一步创建的配置 (Secret) wordpress-secret和 WORDPRESS_DB_PASSWORD。
		点击 添加环境变量，名称填写 WORDPRESS_DB_HOST，值填写 mysql，对应的是上一步创建 MySQL 服务的名称，否则无法连接 MySQL 数据库。
	点击 添加存储卷，选择已有存储卷 wordpress-pvc，访问模式改为 读写，容器挂载路径 /var/www/html。完成后点击 √。
	
5.访问 Wordpress 服务前，查看 wordpress 服务，将外网访问设置为 NodePort。
访问：192.168.56.100:30678 或者任意集群ip:30678都可以  wanzenghui/123456
```

![1659200665516](/assert/1659200665516.png)

![1659200703459](/assert/1659200703459.png)



![1659200960813](/assert/1659200960813.png)

## 3. DevOps工程

### 1.DevOps简介

![1659201663303](/assert/1659201663303.png)

```
存在两种项目，其中上面部署的Wordpress应用属于普通项目，从docker中拉取镜像直接部署应用

DevOps是自己完成从源码到部署的流程（包含推送github的tag，和推送docker hub的镜像tag），通过创建流水线的方式实现
```

![1659248705326](/assert/1659248705326.png)

```
什么是DevOps？
Dev :怎么开发? 怎么承担高并发
Ops:怎么运维? 怎么做到高可用

开发、运维都需要了解的内容，保证开、部署自动化，而不需要运维再部署的时候再来咨询开发如何打包
```

### 2.CICD简介（大图）

**持续集成、持续交付、持续部署**

![1659249061442](/assert/1659249061442.png)

<img src="/1659249632093.png" alt="1659249632093"  />

```
持续集成(Continuous Integration)

集成：jenkins依赖maven build打包（jar），然后部署到环境上
而持续集成需要具备的条件是以下5条
	
	1、持续集成是指软件个人研发的部分向软件整体部分交付,频繁进行集成以便更快地发现其中的错误。"持续集成"源自于极限编程(XP），是XP最初的12种实践之一。
	2、CI需要具备这些:
		1）全面的自动化测试。这是实践持续集成&持续部署的基础，同时，选择合适的自动化测试工具也极其重要;
		2）灵活的基础设施。容器，虚拟机的存在让开发人员和QA人员不必再大费周折;
		3）版本控制工具。如 Git.CVS，SVN等;
		4）自动化的构建和软件发布流程的工具，如Jenkins.flow.ci;
		5）反馈机制。如构建/测试的失败，可以快速地反馈到相关负责人，以尽快解决达到一个更稳定的版本。
```

```
持续交付(Continuous Delivery)

交付：将测试通过的代码交付到 "类生产环境"

	持续交付在持续集成的基础上,将集成后的代码部署到更贴近真实运行环境的「类生产环境」(production-like environments）中。持续交付优先于整个产品生命周期的软件部署，建立在高水平自动化持续集成之上。
【灰度发布。】
持续交付和持续集成的优点非常相似:
	1、快速发布。能够应对业务需求，并更快地实现软件价值。
	2、编码->测试->上线->交付的频繁迭代周期缩短，同时获得迅速反馈;
	3、高质量的软件发布标准。整个交付过程标准化、可重复、可靠.
	4、整个交付过程进度可视化，方便团队人员了解项目成熟度;
	5、更先进的团队协作方式。从需求分析、产品的用户体验到交互设计、开发、测试、运维等角色密切协作，相比于传统的瀑布式软件团队，更少浪费。


```

```
持续部署(Continuous Deployment)

	持续部署是指当交付的代码通过评审之后，白动部署到"生产环境"中。持续部署是持续交付的最高阶段。这意味着，所有通过了一系列的自动化测试的改动都将自动部署到生产环境。它也可以被称为"Continuous Release"。
	
		“开发人员提交代码，持续集成服务器获取代码，执行单元测试，根据测
		试结果决定是否部署到预演环境，如果成功部署到预演环境，进行整体
		验收测试，如果测试通过，自动部署到产品环境，全程自动化高效运转。”
	
	【持续部署主要好处是，可以相对独立地部署新的功能，并能快速地收集真实用户的反馈。】
		"You build ityou run it，这是Amazon一年可以完成 5000万次部署，
		平均每个工程师每天部署超过50次的核心秘籍。
	
```

## 4.对外暴露SonarQube

文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/devops/sonarqube/

```
k8s自动部署了SonarQube，需要对外暴露（新版要自己安装SonarQube）
```

```
先找到服务：kubectl get svc --all-namespaces
	http://192.168.56.100:31688/about
	登录：admin/admin
	Analyze a project：gulimall-analyze
	保留凭证gulimall-analyze: 5fe06110884cba4598249f5d7555b02c1e4ebd3c
	
mvn sonar:sonar \
  -Dsonar.host.url=http://192.168.56.100:31688 \
  -Dsonar.login=5fe06110884cba4598249f5d7555b02c1e4ebd3c

```

![1659252189464](/assert/1659252189464.png)



## 5.Jenkins搭建流水线

文档：https://www.jenkins.io/zh/doc/book/pipeline/

视频：https://www.bilibili.com/video/BV1np4y1C7Yf?p=361

```
DevOps项目，其流水线由Jenkins实现
只要项目内指定好了JenkinsFile，Jenkins就可以按照步骤来持续集成
```

![1659250463213](/assert/1659250463213.png)

```
1、checkout scm：从github拉取代码
2、unit test：单元测试
3、sonarqube analysis：代码审查
4、build and push snapshot：构建镜像推送到docker hub中【登录docker hub查看tag】
5、push the latest image：推送最新镜像【docker hub最新镜像被覆盖】
6、deploy to dev：手动确认是否部署到开发环境【namespace一样的项目】
7、push the tag：手动确认release，代码是否推送到github的tag上【git tag -a v0.0.2 -m v0.0.2】同时0.0.2也会推送一个镜像到docker hub上
8、deploy to production：是否部署到生产环境【namespace一样的项目】
```

案例：https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/devops-online/

```
总结：1.创建凭证（github、docker-hu、sonar-token、demo-kubeconfig）
     2.fork项目到仓库，修改Jenkinsfile-online，保证凭证一致，删除-o；修改Jenkinsfile-online，保证凭证id正确
     3.查看项目内devops-sample-svc.yaml配置，拿到开发环境和生产环境的名称空间
     4.创建资源项目，开发环境、生产环境，名字为上一步拿到的名称空间
     5.创建流水线，选择代码仓库中fork的项目
     6.运行流水线
```



```json
https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/devops-online/
https://www.kubernetes.org.cn/5826.html
需要一个Jenkinsfile ，记录三个凭证
	DockerHub、GitHub 和 kubeconfig (kubeconfig 用于访问接入正在运行的 Kubernetes 集群)
	
1、创建凭证【四个凭证，还有之前记录的SonarQube的token】
	dockerhub-id：账户凭证【存储在self-improve里】
	github-id：账户凭证
	demo-kubeconfig：kubeconfig
	sonar-token：秘密文本，5fe06110884cba4598249f5d7555b02c1e4ebd3c【使用3.0版本自己下载sonar-qube】


2、将项目fork到自己的仓库
https://github.com/kubesphere/devops-java-sample

3、修改Jenkinsfile-online
    environment {
        DOCKER_CREDENTIAL_ID = 'dockerhub-id'
        GITHUB_CREDENTIAL_ID = 'github-id'
        KUBECONFIG_CREDENTIAL_ID = 'demo-kubeconfig'
        REGISTRY = 'docker.io'
        DOCKERHUB_NAMESPACE = 'wanzenghui'
        GITHUB_ACCOUNT = 'wanzenghui'
        APP_NAME = 'devops-java-sample'
        SONAR_CREDENTIAL_ID = 'sonar-token'
    }

4、删除Jenkinsfile-online中maven的-o


5、找到devops-java-sample/deploy/dev/devops-sample-svc.yaml，拿到dev和prod的名称空间：kubesphere-sample-dev 和 kubesphere-sample-prod

6.登录project-admin，创建两个资源型项目，kubesphere-sample-dev、kubesphere-sample-prod，需要与名称空间保持一致，邀请成员project-regular
  创建一个DevOps工程，邀请成员project-regular
	
7、创建流水线
	1）登录project-regular，进入DevOps工程，创建一个流水线demo-cicd
	2）选择一个代码仓库github=》获取token，然后选择fork过来的的项目。下一步【token保存在self-improve】
	3）丢弃分支，使用默认值
	4）指定路径，Jenkinsfile-online
	5）设置拉取时间5min，和webhook回调【在github设置，需要配合内网穿透】
	
8、创建完毕，开始拉取项目、依赖、构建、单元测试
	流水线创建完成后会创建两个活动，因为有两个分支master+dependency
	1）此时会弹出异常，原因：拉取jar包失败
		点开活动 master，查看日志：从repo.spring.io/spring-boot-starter-parent下载失败，应该从maven仓库去下载
	2）解决bug，停止master活动
	修改代码pom.xml中spring-boot-starter-parent的版本【去spring.io查看springboot的版本2.1.13.RELEASE】
	删除<repositories>和<pluginRepositories>
	3）选择流水线，重新运行，tag_name设置v0.0.2
	
9.点击push with tag
	在github上打tag
	在dockerhub上部署版本

10.deploy to production，部署到生产环境
```

**这两步需要手动确认：**

![1659265641504](/assert/1659265641504.png)

**github：**

![1659265493628](/assert/1659265493628.png)

**dockerhub：**

![1659265524650](/assert/1659265524650.png)



